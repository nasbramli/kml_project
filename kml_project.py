# -*- coding: utf-8 -*-
"""kml_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gJmdYhcD1n3ndMDE0yd30AYRf4I4nmJS

# Final Project Template

This workbook provides the template for the final project.

## Instructions
- Work individually or in pairs
- Each team is to complete 1 copy of this template.
  - Complete all sections.
  - Feel free to include supporting material / slides / documents as needed.
- At the end of the project, you will get 5 minutes to present this workbook to the class.

### Submission Instructions
- Submit the .ipynb with the Output cells showing the results
  - Naming convention:
  ```
      <name1>-<name2>-<project_short_name>.ipynb
  ```
- If you provide your own datasets, include the data with your .ipynb, unless it is confidential
"""

from google.colab import drive
drive.mount('/gdrive')

"""## Section 0: Team Members
- Jing Jie Lim
- Nasruddin Islam Bin Ramli
- Ranjan Batra

## Section 1: Graduate Admission Prediction

- Graduate admission prediction based on applicant's scores, gpa and the university's rating

## Section 2: Project Definition

### Goals

Describe the goal of this project:

Predict graduate admission based on applicants' scores/gpa and the university's rating.

Important:
- If this is your first project, keep the project definition as simple as possible.
- As a rule of thumb, pick something that can be completed in 2-3 hours.
- If you are not sure, use the workshop problems as a reference.

### Dataset

URL: https://www.kaggle.com/mohansacharya/graduate-admissions  
Attributes:
- GRE Score
- TOEFL Score
- University Rating
- SOP
- LOR
- CGPA
- Research
- Target: Chance of Admit

### Tasks

List the tasks you will perform.

1. Explore the data
2. Determine balance of data
3. Shuffle and split into train and test sets
4. Train classification model using SGDClassifier, SVC
5. Compute classification report and plot confusion matrix for all models
6. Perform analysis for possible improvements

## Section 3: Data Engineering

Consider:
* Data Cleaning
* Data Exploration
* Imbalance / Data Sampling
* Data Encoding
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import SGDClassifier
from sklearn.svm import SVC
from sklearn.linear_model import SGDRegressor
from sklearn.model_selection import train_test_split, learning_curve, GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, accuracy_score

df = pd.read_csv('/gdrive/My Drive/kml-workshop/Admission_Predict.csv')
df.info()

df1= df.drop(['Serial No.'], axis = 1)
df1.loc[df1['Chance of Admit ']> 0.7, 'Admitted'] = 1
df1.loc[df1['Chance of Admit ']<= 0.7, 'Admitted'] = 0
df1.head()

df1.corr()

fig, axes = plt.subplots(figsize=(10, 10))
sns.heatmap(df1.corr(), ax=axes, annot=True, fmt='.2f')

X = df1.drop(['Chance of Admit ', 'Admitted'], axis = 1)
y = df1['Admitted']

"""## Section 4: Feature Engineering

Consider:
* Feature Reduction
* PCA plot in 2D
"""

from sklearn.decomposition import PCA

# scale
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# pca
pca_2d = PCA(n_components=2)
Z_2d = pca_2d.fit_transform(X_scaled)

fig, ax = plt.subplots()

# dimension 0, dimension 1, label
ax.scatter(Z_2d[y==0, 0], Z_2d[y==0, 1], label='Not Admitted')
ax.scatter(Z_2d[y==1, 0], Z_2d[y==1, 1], label='Admitted')
ax.set(title='PCA 2-d projection', xlabel='Z[0]', ylabel='Z[1]')
ax.legend()

"""## Section 5: Model Engineering

Consider:
* Learning Curve to avoid Overfitting
"""

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)

# fit a separate X_scaler on the training set only
X_scaler = StandardScaler()
X_scaler.fit(X_train)

Z_train = X_scaler.transform(X_train)
Z_test = X_scaler.transform(X_test)

sgd = SGDClassifier(tol=1e-3, random_state=42)
sample_sizes, train_scores, val_scores = learning_curve(sgd, Z_train, y_train, cv=3, verbose=1, random_state=42)

fig, ax = plt.subplots()

# Note: for classification model, scores are accuracy
# (for regression model, scores are r2)
ax.plot(sample_sizes, train_scores.mean(axis=1), label='train')
ax.plot(sample_sizes, val_scores.mean(axis=1), color='orange', label='validation')
ax.set(xlabel='samples', ylabel='accuracy', title='Learning curve (SGD Logistic Regression)')
ax.legend()

"""## Section 6: Evaluate Metrics

Consider:
* Which metric to use?
"""

sgd = SGDClassifier(tol=1e-3, random_state=42)
sgd.fit(Z_train, y_train)

pred_sgd = sgd.predict(Z_test)
print(classification_report(y_test, pred_sgd))
ax = sns.heatmap(confusion_matrix(y_test, pred_sgd), annot=True, fmt='d')
ax.set(xlabel='Prediction', ylabel='Truth')

# fit SVM classifier on all samples (based on learning curve)
svc = SVC(gamma='auto', random_state=42)
svc.fit(Z_train, y_train)

pred_svc = svc.predict(Z_test)
print(classification_report(y_test, pred_svc))

ax = sns.heatmap(confusion_matrix(y_test, pred_svc), annot=True, fmt='d')
ax.set(xlabel='Prediction', ylabel='Truth')

"""## Section 7: Observations and analysis

Answer the following questions:
1. What do you conclude from the metrics?

2. What improvements do you propose?

Based on the classification report, the model performs generally well on both classes (Admitted & Not Admitted). However, the precision for the 'Not Admitted' class is slightly low. This means the amount of false negatives is fairly high.

Some improvements that can be made is utilizing grid search to optimize/tune the hyperparameters of our baseline model. If no improvements are seen, we would suggest collecting more data (of both classes).
"""